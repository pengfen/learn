[root@peng1 ~]# cd /home/spark/
[root@peng1 spark]# ./bin/spark-shell
scala> val rdd = sc.textFile("hdfs://peng1:9000/usr/input/wc/wc-in")
rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at <console>:27
scala> val flat = rdd.flatMap(_.split(" "))
flat: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:29
scala> val word = flat.map((_, 1))
word: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:31
scala> val redecer = word.reduceByKey(_ + _)
redecer: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:33
scala> redecer.collect
res2: Array[(String, Int)] = Array((put,4), (hadoop,2), (get,3), (hello,3), (welcom,1), (welcome,3), (world,2))


scala> val rdd = sc.textFile("/usr/input/wc/wc-in")
rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at textFile at <console>:27

scala> rdd.first
res3: String = welcome hadoop welcome hello
scala> rdd.count
res4: Long = 6

# print 打印  println 打印并换行
scala> print("Answer : ")
Answer :
scala> println(42)
42

scala> println("Answer : " + 42)
Answer : 42

# eclipse 上测试输入内容
object ReadLine {
  def main(args: Array[String]): Unit = {
    val name = readLine("请输入你的名字 : ")
    print("请输入你的年龄 : ")
    val age = readInt()
    printf("Hello, %s! 下一年, 你是 %d 岁\n", name, age + 1)
  }
}

def main(args: Array[String]): Unit = {
    this.whileTest();
  }
  
  def whileTest() { // 测试 while 循环
    var n = 10
    var r = 1
    while ( n > 0 ) {
      r = r * n
      n = n - 1
      println(s"r 的值是 : ${r} \t\t n 的值是 : ${n}")
    }
  }
