代码

show databases;
use default; # 不使用 use 也是进入默认数据库
show talbes;
desc fdm_ord_
drop table test_imooc;
create table test_imooc (id bigint, name string);
insert overwrite table test_imooc slect 100,'helloword' from default;
select * from test_imooc;

show tables 显示所有表
show partitions 显示所有分区
show table extended 显示表的扩展信息
show table properties 显示所有表的属性
show indexes 显示所有索引
show create table 显示创建表的语句
show databases 显示数据库
show columns 显示所有列
show functions 显示所有函数

show granted roles and privileges 显示所有角色与权限
show locks 显示锁
show conf 显示配置文件
show transactions 显示事务

show databases like '*dm*';
show partitions table_name;
show partitions table_name parition(name='value');

create index index_name on table table_name(column_name) as 'compact' with deferred rebuild;

show index on table_name;

dorp index index_name on table_name;

show columns from table_name;

show functions;
show functions '.*f*.';

describe database 显示某数据库的详情
describe table/view/column 显示表 视图 列的详情
display column statistics 0.14 才支持
describe partition 显示分区的详情

desc table_name.column_name;


hive 的命令分以下几类
进入与退出 Hive 交互 hive quit exit
参数设置 set reset
资源文件管理 add list delete
执行 shell 命令 !cmd
hdfs 文件操作 dfs-ls dfs-cat
hiveql
执行外部文件 source file
compile


set; # 显示所有
set -v; # 显示所有

set mapred.reduce.tasks;
set mapred.reduce.tasks=30;
reset; # 所有参数设置为默认值
reset mapred.reduce.tasks;

hive -e "set;" | grep tasks;
!ls; # 执行外部 shell 不支持管道符等交互式的命令
!clear;
dfs -ls /usr/
dfs -mkdir /usr/input
dfs ...

add file /root/hadoop/a.sql;
list file;
delete file /root/hadoop/a.sql;

-d, --defina<key=value> # 定义一个变量在 hive 中使用
-e, <quoted-query-string> # 执行单引号或者双引号内的 SQL
-f <filename> 执行一个 SQL 文件中的 SQL
-H, --help # 打印帮助信息
-i <filename> # 执行初始化文件
-p # 连接 Hive 服务器的端口号
-h <hostname> # 连接到远程主机
--hiveconf <property=value> #设置属性
--hivevar <key=value> # 设置变量
-S 静默模式 不会打印一些日志
-v, --verbose # 冗余模式 会打印出执行的语句和执行日志

hive -h;
hive -e "select * from table_name limit 10";
hive -e "select * from table_name limit 10" > aa.txt;
cat aa.txt;
vi b.sql;
hive -f b.sql >> ee.txt;
wc ee.txt
hive -v -e "";

hive --database database_name;



数据仓库基础表概览
数据仓库已经做好的工作
数据已经从源数据库抽取装载到 BDM 层
FDM 层根据 BDM 层已经填入按天分区的数据
没有每天的调度 也没有辅助系统 都是手动生成的数据
数据是造的测试数据 只有 DT=2015-01-01 的

BDM 层数据表 (贴源的缓冲表)
bdm_b2c_active 活动表
...order 订单表
...active_active 活动订单表
...order_desc 订单明细表
...order_goods 订单商品表
...user 用户表
...cart 购物车表

FDM 层数据表 (拉链表 或者按天分区的表)
fdm_act_main 活动主表
..._order_active 活动订单表
..._app_view app 端 view 表
..._log_pageview PC端PV表
..._log_pc_view PC端view表
..._ord_cart 购物车表
..._ord_order 订单表
..._ord_order_desc 订单明细表
..._ord_order_goods 订单商品表
..._ord_goods 商品表
..._user_wide 用户宽表

GDM 层 (通用数据模型层)
gdm_ord_order 订单模型表


数据开发前置依赖
需求确定
建模确定表结构
实现方案确定

数据开发过程
表落地
按表写 SQL 代码生成数据
部署代码
数据测试
试运行与上线

举例 订单模型表开发过程
表落地 gdm_ord_order
按表写 SQL 代码生成数据
部署代码 gdm_order_order.sh
数据测试
试运行与上线

desc gdm_order_order

#************
# 文件名称: gdm_order_order.sh
# 功能描述: 订单模型表
# 创建者  : apeng
# 创建日期: 5016-03-22
# 修改日期:   修改人:   修改内容
#************
#!/bin/bash
DT=`date -d '-1 day' "+%Y-%m-%d"`

sysdate = `date "+%Y-%m-%d"`

if [ $1];then
    DT=$1
fi

SQL = "
insert overwrite table gdm.gdm_ord_order partition(dt='"${DT}"')
select a.order_id,
...
s.last_update_time, -- 最后修改时间
form_unixtime(unix_timestamp()) dw_date
form (select * from fdm.fdm_ord_order where dt = '"${DT}"') a
join (select * from fdm.fdm_ord_order_desc where dt = '"${DT}"') b
on (a.order_id = b.order_id);

"
echo "${SQL}"

hive -e "${SQL}"



jdbc 代码

启动 Hive 远程服务
hive --service hiveserver
获取连接 --- 创建运行环境 --- 执行 HQL --- 处理结果 --- 释放资源

创建项目
创建 lib --- 复制第三方jar 到 (hive-jdbc-....jar) --- add build path

public class JDBCUtils {
	private static String driver = "org.apache.hadoop.hive...";
	private static String url = "jdbc:hive:/peng1/default";

	// 注册驱动
	static {
	    try {
	        Class.forName(driver);
	    } catch (ClassNotFoundException e) {
	        throw new ExceptionInitializerError(e);
	    }
	}

	// 获取连接
	public static Connection getConnection() {
	    try {
	        return DriverManager.getConnection(url);
	    } catch (SQLException e) {
	        e.printStackTrace();
	    }
	    return null;
	}

	// 释放资源
	public static void release(Connection conn, Statement st, ResultSet rs) {
	    if (rs != null) {
	        try {
	            rs.close();
	        } catch (SQLException e) {
	            e.printStackTrace();
	        } finally {
	            rs = null;
	        }
	    }

	    if (st != null) {
	        try {
	            st.close();
	        } catch (SQLException e) {
	            e.printStackTrace();
	        } finally {
	            st = null;
	        }
	    }

	    if (conn != null) {
	        try {
	            conn.close();
	        } catch (SQLException e) {
	            e.printStackTrace();
	        } finally {
	            conn = null;
	        }
	    }
	}
}

public class HiveJDBCDemo {
	public static void main(String[] args) {
	    Connection conn = null;
	    Statement st = null;
	    ResultSet rs = null;
	    String sql = "select * from emp";
	    try {
	        // 获取连接
	        conn = JDBCUtils.getConnection();
	        // 创建运行环境
	        st = conn.createStatement();
	        // 运行 HQL
	        rs = st.executeQuery(sql);
	        // 处理数据
	        while (rs.next()) {
	            // 取出员工的姓名
	            String name = rs.getString(2);
	            double sal = rs.getDouble(6);
	            System.out.println(name + '\t' + sal);
	        }
	    } catch (Exception e) {
	        e.printStackTrace();
	    } finally {
	        JDBCUtils.release(conn, st, rs);
	    }
	}
}


使用 Thire 客户端操作
public class HiveThriftClient {
	public static void main(String[] args) throws Exception {
	    // 创建 Socket: 连接
	    final TSocket tSocket = new TSocket("192.168...", 10000);

	    // 创建一个协议
	    final TProtocol tProtcal = new TBinaryProtocol(tSocket);

	    // 创建 hive Client
	    final HiveClient client = new HiveClient(tProtcal);

	    // 打开 Socket
	    tSocket.open();

	    // 执行 HQL
	    client.execute("desc emp");

	    // 处理结果
	    List<String> columns = client.fetchAll();

	    for (String col:columns) {
	        System.out.println(col);
	    }

	    // 释放资源
	    tSocket.close();
	}
}

insert into table t_test values (1, 'peng1'),(2, 'peng2')