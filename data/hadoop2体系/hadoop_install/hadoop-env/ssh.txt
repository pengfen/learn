配置无密码登录 peng2 peng3

[root@peng1 ~]# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
[root@peng1 ~]# cd /root/.ssh
[root@peng1 .ssh]# scp id_dsa.pub root@peng2:~
[root@peng1 .ssh]# scp id_dsa.pub root@peng3:~

登录 peng2
[root@peng2 ~]# ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
[root@peng2 ~]# cat id_dsa.pub >> ~/.ssh/authorized_keys
[root@peng2 ~]# more ~/.ssh/authorized_keys
[root@peng2 ~]# mkdir /root/hadoop/

登录 peng3 同上操作

复制 JDK 环境
[root@peng1 ~]# scp -r /usr/local/java root@peng2:/usr/local/
[root@peng1 ~]# scp -r /usr/local/java root@peng3:/usr/local/
[root@peng1 ~]# scp -r /etc/profile root@peng2:/etc/
[root@peng1 ~]# scp -r /etc/profile root@peng3:/etc/

登录 peng2 (检测 java 环境)
[root@peng2 ~]# vi /etc/profile 
[root@peng2 ~]# source /etc/profile
[root@peng2 ~]# java -version

登录 peng3 同上操作

复制 Hadoop 环境
[root@peng1 ~]# scp -r /etc/hosts root@peng2:/etc/
[root@peng1 ~]# scp -r /etc/hosts root@peng3:/etc/
[root@peng1 ~]# scp -r /root/hadoop/hadoop-1... root@peng2:/root/hadoop/
[root@peng1 ~]# scp -r /root/hadoop/hadoop-1... root@peng3:/root/hadoop/

登录 peng2 (检测 hadoop 环境)
[root@peng2 ~]# vi /etc/hosts
[root@peng2 ~]# ln -sf /root/hadoop/hadoop-1... /home/hadoop1.2
[root@peng2 ~]# cd /home/hadoop1.2/
[root@peng2 hadoop1.2]# cd conf
[root@peng2 conf]# cd /home/hadoop1.2/
[root@peng2 conf]# vi hadoop-env.sh
[root@peng2 conf]# vi core-site.xml
[root@peng2 conf]# vi hdfs-site.xml
[root@peng2 conf]# vi mapred-site.xml
[root@peng2 conf]# vi masters
[root@peng2 conf]# vi slaves

登录 peng3 同上操作

关闭三台电脑的防火墙
service iptables stop

启动 hadoop 运行测试用例
[root@peng1 ~]# cd /home/hadoop1.2/
[root@peng1 hadoop1.2]# ./bin/hadoop namenode -format
[root@peng1 hadoop1.2]# ./bin/start-all.sh
[root@peng1 hadoop1.2]# jps

登录 peng2
[root@peng2 ~]# jps

登录 peng3 同上操作

一切正常
windows .../etc/hosts 中配置
192.168.129.128 peng1
192.168.129.129 peng2
192.168.129.130 peng3
http://peng1:50070
http://peng1:50030

运行第一个测试样例
[root@peng1 hadoop1.2]# ./bin/hadoop jar /home/hadoop1.2/hadoop-examples-1.2.1.jar pi 10 100
Number of Maps  = 10
Samples per Map = 100
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Wrote input for Map #3
Wrote input for Map #4
Wrote input for Map #5
Wrote input for Map #6
Wrote input for Map #7
Wrote input for Map #8
Wrote input for Map #9
Starting Job
16/03/14 02:18:52 INFO mapred.FileInputFormat: Total input paths to process : 10
16/03/14 02:18:53 INFO mapred.JobClient: Running job: job_201603140217_0001
16/03/14 02:18:54 INFO mapred.JobClient:  map 0% reduce 0%
16/03/14 02:19:02 INFO mapred.JobClient:  map 20% reduce 0%
16/03/14 02:19:03 INFO mapred.JobClient:  map 40% reduce 0%
16/03/14 02:19:08 INFO mapred.JobClient:  map 60% reduce 0%
16/03/14 02:19:09 INFO mapred.JobClient:  map 80% reduce 0%
16/03/14 02:19:12 INFO mapred.JobClient:  map 100% reduce 0%
16/03/14 02:19:14 INFO mapred.JobClient:  map 100% reduce 26%
16/03/14 02:19:16 INFO mapred.JobClient:  map 100% reduce 100%
16/03/14 02:19:16 INFO mapred.JobClient: Job complete: job_201603140217_0001
16/03/14 02:19:16 INFO mapred.JobClient: Counters: 30
16/03/14 02:19:16 INFO mapred.JobClient:   Map-Reduce Framework
16/03/14 02:19:16 INFO mapred.JobClient:     Spilled Records=40
16/03/14 02:19:16 INFO mapred.JobClient:     Map output materialized bytes=280
16/03/14 02:19:16 INFO mapred.JobClient:     Reduce input records=20
16/03/14 02:19:16 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=3219927040
16/03/14 02:19:16 INFO mapred.JobClient:     Map input records=10
16/03/14 02:19:16 INFO mapred.JobClient:     SPLIT_RAW_BYTES=1170
16/03/14 02:19:16 INFO mapred.JobClient:     Map output bytes=180
16/03/14 02:19:16 INFO mapred.JobClient:     Reduce shuffle bytes=280
16/03/14 02:19:16 INFO mapred.JobClient:     Physical memory (bytes) snapshot=1476296704
16/03/14 02:19:16 INFO mapred.JobClient:     Map input bytes=240
16/03/14 02:19:16 INFO mapred.JobClient:     Reduce input groups=20
16/03/14 02:19:16 INFO mapred.JobClient:     Combine output records=0
16/03/14 02:19:16 INFO mapred.JobClient:     Reduce output records=0
16/03/14 02:19:16 INFO mapred.JobClient:     Map output records=20
16/03/14 02:19:16 INFO mapred.JobClient:     Combine input records=0
16/03/14 02:19:16 INFO mapred.JobClient:     CPU time spent (ms)=5790
16/03/14 02:19:16 INFO mapred.JobClient:     Total committed heap usage (bytes)=1623891968
16/03/14 02:19:16 INFO mapred.JobClient:   File Input Format Counters
16/03/14 02:19:16 INFO mapred.JobClient:     Bytes Read=1180
16/03/14 02:19:16 INFO mapred.JobClient:   FileSystemCounters
16/03/14 02:19:16 INFO mapred.JobClient:     HDFS_BYTES_READ=2350
16/03/14 02:19:16 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=597830
16/03/14 02:19:16 INFO mapred.JobClient:     FILE_BYTES_READ=226
16/03/14 02:19:16 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=215
16/03/14 02:19:16 INFO mapred.JobClient:   File Output Format Counters
16/03/14 02:19:16 INFO mapred.JobClient:     Bytes Written=97
16/03/14 02:19:16 INFO mapred.JobClient:   Job Counters
16/03/14 02:19:16 INFO mapred.JobClient:     Launched map tasks=10
16/03/14 02:19:16 INFO mapred.JobClient:     Launched reduce tasks=1
16/03/14 02:19:16 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=13250
16/03/14 02:19:16 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
16/03/14 02:19:16 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=55034
16/03/14 02:19:16 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
16/03/14 02:19:16 INFO mapred.JobClient:     Data-local map tasks=10
Job Finished in 23.662 seconds
Estimated value of Pi is 3.14800000000000000000
